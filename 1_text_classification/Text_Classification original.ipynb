{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de textes\n",
    "                                            Emanuela Boros\n",
    "                                            Université de La Rochelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "La classification des objets consiste à attribuer une classe à un objet. Ce objet peut être de type texte, image, audio ou vidéo. Pour savoir comment classifier un objet, il est important de connaître les caracteristiques qui definissent une classe. Par exemple, si on considere les caracteristiques : quantités de roues, selle, guidon, et frein. Un vélo ordinaire est composé de deux roues, une selle et un guidon. Une voiture a 4 roues, des freins mais pas de selle ni cadre ni guidon. Une moto est aussi composée de deux roues, une selle, des friens et un guidon. Avec ces caracteristiques, la moto et le vélo ont la même répresentation.\n",
    "\n",
    "Nous pourrions compliquer encore plus cette tâche et essayer de classifier les chihuahuas ou muffins suivants. Pour faire cela, quelles sont les caracteristiques pertinentes à analyser ?\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"images/classification.jpeg\" width=\"400\" height=\"600\">\n",
    "    <figcaption>Source: <a href=\"https://www.freecodecamp.org/news/chihuahua-or-muffin-my-search-for-the-best-computer-vision-api-cbda4d6b425d/\">Link</a></figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "Vous trouvez ça difficile ? Vous avez raison.\n",
    "\n",
    "La quantité et la pertinence des caracteristiques peuvent aider à classifier des objets. Plus les caracteristiques sont indépendantes et différentes, plus on aura d'informations complementaires sur l'objet analysé.\n",
    "\n",
    "En suivant cette méthode, nous analyserons dans ce cours la classification de textes. Il est très important de faire attention aux caracteristiques pour chaque objet (dans ce cas : les mots et leurs caractéristiques). Notre objectif est d'avoir des caracteristiques pertinentes pour mieux définir un objet et pouvoir l'attribuer à une catégorie correctement.\n",
    "\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"images/text_processing_flow.png\" width=\"700\" height=\"100\">\n",
    "    <figcaption>Source: <a href=\"https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/\">Link</a></figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Dans ce cours, nous explorerons un ensemble de données, une analyse de texte et plusieurs approches basées sur l'apprentissage automatique et profond, de la manière suivante:\n",
    "\n",
    "- **Récupération** du corpus de textes (*.csv, *.txt, *.json, etc.)\n",
    "- **Prétraitement** des données textuelles (text pre-processing) : tokenisation etc\n",
    "- **Exploration** du corpus (*EDA*, exploratory data analysis) : analyse des fréquences\n",
    "- **Representation** des mots (bag of words, *sac de mots*, TF-IDF, plongements de mots, *word embeddings*)\n",
    "- **Apprentissage automatique** (machine learning) et **apprentissage profond** (deep learning)\n",
    "- **Analyse d'erreurs** (evaluation, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Récupération, Prétraitement et Exploration du corpus de textes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 1. Contexte\n",
    "\n",
    "Le gouvernement américain a attaqué en justice cinq grands groupes américains du tabac pour avoir amassé d'importants bénéfices en mentant sur les dangers de la cigarette. Le cigarettiers  se sont entendus dès 1953, pour \"mener ensemble une vaste campagne de relations publiques afin de contrer les preuves de plus en plus manifestes d'un lien entre la consommation de tabac et des maladies graves\". \n",
    "\n",
    "Dans ce procès 6,910,192 de documents ont été collectés et numérisés. Afin de faciliter l'exploitation de ces documents par les avocats, vous êtes en charge de mettre en place une classification automatique des types de documents: **Advertisement, Email, Form, Letter, Memo, News, Note, Report, Resume, Scientific**.\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>Letter</td>\n",
    "     <td>News</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"images/50661869-1869.jpg\" width=270 height=480></td>\n",
    "    <td><img src=\"images/10031617.jpg\" width=270 height=480></td>\n",
    "  </tr>\n",
    " </table>\n",
    "\n",
    "\n",
    "Un échantillon aléatoire des documents a été collecté et des opérateurs ont classé les documents dans des répertoires correspondant aux classes de documents : lettres, rapports, notes, email, etc. Vous avez à votre disposition : \n",
    "\n",
    "- le texte contenu dans les documents obtenu par OCR (en anglais : optical character recognition; signifie reconnaissance optique de caractères ou reconnaissance de texte, une technologie qui permet de convertir différents types de documents tels que les documents papiers scannés en fichiers modifiables - texte); <ins>**path**</ins>\n",
    "- les classes des documents définies par des opérateurs; <ins>**label**</ins>\n",
    "\n",
    "Aprés téléchargement et extraction du fichier compressé, nous pourrons charger et regarder le structure de ces commentaires avec la bibliothèque pandas (plus d'info [```pandas```](https://pandas.pydata.org/docs/user_guide/index.html)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # https://pandas.pydata.org/docs/\n",
    "\n",
    "data = pd.read_csv('data/Tobacco3482-text/tobacco_texts.csv')#.sample(frac=.5) # lire le fichier .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1392"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>data/Tobacco3482-text/Scientific/2051025161.txt</td>\n",
       "      <td>Scientific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>data/Tobacco3482-text/Note/87705667.txt</td>\n",
       "      <td>Note</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>data/Tobacco3482-text/Scientific/50590463-0469...</td>\n",
       "      <td>Scientific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>data/Tobacco3482-text/Resume/50617225-7226.txt</td>\n",
       "      <td>Resume</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>data/Tobacco3482-text/News/2078115137.txt</td>\n",
       "      <td>News</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path       label\n",
       "0    data/Tobacco3482-text/Scientific/2051025161.txt  Scientific\n",
       "1            data/Tobacco3482-text/Note/87705667.txt        Note\n",
       "2  data/Tobacco3482-text/Scientific/50590463-0469...  Scientific\n",
       "3     data/Tobacco3482-text/Resume/50617225-7226.txt      Resume\n",
       "4          data/Tobacco3482-text/News/2078115137.txt        News"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head() # afficher les 5 premières lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>691</td>\n",
       "      <td>data/Tobacco3482-text/Note/12882100.txt</td>\n",
       "      <td>Note</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>692</td>\n",
       "      <td>data/Tobacco3482-text/Form/2505233645_3646.txt</td>\n",
       "      <td>Form</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>693</td>\n",
       "      <td>data/Tobacco3482-text/Memo/2048153405_20481534...</td>\n",
       "      <td>Memo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>694</td>\n",
       "      <td>data/Tobacco3482-text/Scientific/50492374-2377...</td>\n",
       "      <td>Scientific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>695</td>\n",
       "      <td>data/Tobacco3482-text/Form/2505106228.txt</td>\n",
       "      <td>Form</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  path       label\n",
       "691            data/Tobacco3482-text/Note/12882100.txt        Note\n",
       "692     data/Tobacco3482-text/Form/2505233645_3646.txt        Form\n",
       "693  data/Tobacco3482-text/Memo/2048153405_20481534...        Memo\n",
       "694  data/Tobacco3482-text/Scientific/50492374-2377...  Scientific\n",
       "695          data/Tobacco3482-text/Form/2505106228.txt        Form"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail() # afficher les 5 dernières lignes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec Pandas, nous pouvons commencer avec quelques statistiques simples. Les résultats suivants seront dans l'ordre décroissant de sorte que le premier élément soit l'élément le plus fréquent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec Pandas, nous pouvons également visualiser ces statistiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = data.label.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons vu que le fichier .csv contenait les chemins et les étiquettes des documents. Maintenant, nous devons lire les fichiers à partir des chemins spécifiés pour obtenir les données textuelles.\n",
    "\n",
    "**<span style=\"color:red\">To do</span>**:\n",
    "\n",
    "> * Pour chaque chemin dans le dataframe, lisez le fichier associé et ajoutez les textes dans un vecteur. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "texts = []\n",
    "for idx, line in data.iterrows(): # itérer dans un dataframe Pandas\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "data['text'] = texts # créer une nouvelle colonne dans le dataframe Pandas avec les textes associés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/Tobacco3482-text/Scientific/2051025161.txt</td>\n",
       "      <td>Scientific</td>\n",
       "      <td>MONTHLY SUMMARY\\n\\n25 JANUARY 1986-\\n24 FEBRUA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/Tobacco3482-text/Note/87705667.txt</td>\n",
       "      <td>Note</td>\n",
       "      <td>2/14/90\\n\\n   \\n\\nDATE:\\n\\n \\n\\n  \\n \\n\\nSUBJE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/Tobacco3482-text/Scientific/50590463-0469...</td>\n",
       "      <td>Scientific</td>\n",
       "      <td>Cell, Vol. 59, 1107-1113, December 22, 1989, C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/Tobacco3482-text/Resume/50617225-7226.txt</td>\n",
       "      <td>Resume</td>\n",
       "      <td>BIOGRAPHICAS SKETCH\\n\\nGive the following info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/Tobacco3482-text/News/2078115137.txt</td>\n",
       "      <td>News</td>\n",
       "      <td>Tae NEWS &amp; OBSERVER\\n\\nFaday, Ocroagr 15, 1999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data/Tobacco3482-text/Memo/96008379.txt</td>\n",
       "      <td>Memo</td>\n",
       "      <td>TOBACCO COMPANY\\n\\n24300 CATHERINE INDUSTRIAL ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>data/Tobacco3482-text/Report/10176813.txt</td>\n",
       "      <td>Report</td>\n",
       "      <td>PB ES FOOT\\n\\nGRANT APPLICATION No. 814A\\n\\nSu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>data/Tobacco3482-text/Memo/94011032_94011033.txt</td>\n",
       "      <td>Memo</td>\n",
       "      <td>THE TOBACCO INSTITUTE\\n\\n1875 | STREET. NORTHW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>data/Tobacco3482-text/Letter/502406921_5024069...</td>\n",
       "      <td>Letter</td>\n",
       "      <td>Loe ey PYF at nn peg py pee ry 84\\nwipe rPVOUU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>data/Tobacco3482-text/Memo/2046000414_20460004...</td>\n",
       "      <td>Memo</td>\n",
       "      <td>- PHILIP MORRIS COMPANIES INC, _ INTER-OFFICE\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path       label  \\\n",
       "0    data/Tobacco3482-text/Scientific/2051025161.txt  Scientific   \n",
       "1            data/Tobacco3482-text/Note/87705667.txt        Note   \n",
       "2  data/Tobacco3482-text/Scientific/50590463-0469...  Scientific   \n",
       "3     data/Tobacco3482-text/Resume/50617225-7226.txt      Resume   \n",
       "4          data/Tobacco3482-text/News/2078115137.txt        News   \n",
       "5            data/Tobacco3482-text/Memo/96008379.txt        Memo   \n",
       "6          data/Tobacco3482-text/Report/10176813.txt      Report   \n",
       "7   data/Tobacco3482-text/Memo/94011032_94011033.txt        Memo   \n",
       "8  data/Tobacco3482-text/Letter/502406921_5024069...      Letter   \n",
       "9  data/Tobacco3482-text/Memo/2046000414_20460004...        Memo   \n",
       "\n",
       "                                                text  \n",
       "0  MONTHLY SUMMARY\\n\\n25 JANUARY 1986-\\n24 FEBRUA...  \n",
       "1  2/14/90\\n\\n   \\n\\nDATE:\\n\\n \\n\\n  \\n \\n\\nSUBJE...  \n",
       "2  Cell, Vol. 59, 1107-1113, December 22, 1989, C...  \n",
       "3  BIOGRAPHICAS SKETCH\\n\\nGive the following info...  \n",
       "4  Tae NEWS & OBSERVER\\n\\nFaday, Ocroagr 15, 1999...  \n",
       "5  TOBACCO COMPANY\\n\\n24300 CATHERINE INDUSTRIAL ...  \n",
       "6  PB ES FOOT\\n\\nGRANT APPLICATION No. 814A\\n\\nSu...  \n",
       "7  THE TOBACCO INSTITUTE\\n\\n1875 | STREET. NORTHW...  \n",
       "8  Loe ey PYF at nn peg py pee ry 84\\nwipe rPVOUU...  \n",
       "9  - PHILIP MORRIS COMPANIES INC, _ INTER-OFFICE\\...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10) # visualisez les donnees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple tiré de l'ensemble de données Tobacco:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classe :', data['label'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Étant donné que cet ensemble de données a été numérisé, des problèmes peuvent apparaître avec les données. Il n'y a toujours pas d'outils OCR qui fonctionnent au niveau humain dans la plupart des applications. L'existence aujourd'hui de plusieurs outils de ce type a conduit peu à peu à définir des critères de choix pour sélectionner l'OCR le plus efficace et surtout le mieux adapté à son application. Longtemps, le critère d'efficacité était lié à un taux de reconnaissance élevé, pensant qu'une technologie efficace est une technologie sans défaut. En effet, il faut admettre quele taux de 100% reste un objectif à atteindre.\n",
    "\n",
    "Les erreurs incluent une mauvaise lecture des lettres, le saut de lettres illisibles ou la combinaison de texte de colonnes adjacentes ou de légendes d'image. Bien que de nombreux facteurs affectent les performances des outils OCR, le nombre d'erreurs dépend de la qualité et de la forme du texte, y compris du font utilisé.\n",
    "\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"images/1004859787.jpg\" width=\"300\" height=\"100\">\n",
    "    <figcaption>News</figcaption>\n",
    "    </center>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un exemple d'un tel document, avec de nombreuses erreurs de reconnaissance de caractères et de mots :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['text'][57])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classe :', data['label'][57])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La figure correspond au texte et il est visible que sa qualité est faible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Pré-traitement de texte \n",
    "\n",
    "Dans tout processus d'analyse de texte, la première et la plus importante des étapes est de **constituer un corpus**, un ensemble de documents, et de **le nettoyer** afin de l'exploiter. \n",
    "\n",
    "Les étapes de prétraitement (*preprocessing*) sont cruciales car elles permettent de **nettoyer le texte** de tous ses éléments qui ne sont **pas porteurs de sens**, et le préparent pour l’analyse.\n",
    "Les différentes étapes sont :\n",
    "1. La ***tokenisation*** est un moyen de séparer un morceau de texte en unités plus petites appelées **tokens**. Les **tokens** sont généralement des **mots**.\n",
    "\n",
    "2. La ***normalisation*** du texte est importante pour les textes bruyants comme par exemple ces données. qui sont des documents numérisés qui, comme nous l'avons vu, contiennent de nombreuses erreurs: les **abréviations**, les **fautes d'orthographe** et l'utilisation de mots hors vocabulaire (oov) sont répandus. Ce procédé implique également la **suppression des mots vides** ou des **mots bruyants** qui peuvent perturber l'analyse (conjugaisons, majuscules, ponctuations, etc.).\n",
    "\n",
    "**Note**: en analyse de données, ces étapes sont généralement les plus fastidieuses, car elle impliquent un long travail de normalisation de données.\n",
    "\n",
    "Nous allons utiliser les bibliothèques de traitement du langage [`nltk`](https://www.nltk.org/), [`spaCy`](https://spacy.io) ainsi que les bibliothèques scientifiques classiques que sont [`pandas`](https://pandas.pydata.org/), [`numpy`](https://numpy.org/). La bibliothèque [`spaCy`](https://spacy.io) nous permet d'obtenir ces informations facilement.\n",
    "\n",
    "PS: l'installation de cette bibliothèque est très simple dans envirement virtuel (ou ici, dans le notebook):\n",
    "\n",
    "```\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Suppression des [mots vides](https://fr.wikipedia.org/wiki/Mot_vide) (stopwords)\n",
    "\n",
    "La première manipulation souvent effectuée dans le traitement du texte est la suppression des mots vides, ou *stopwords*. Ce sont les mots très courants dans la langue étudiée (« the », « a », en anglais) qui dans beaucoup des cas **n'apportent pas de valeur informative** pour la compréhension d’un document ou d’un corpus. \n",
    "\n",
    "Par exemple, dans le contexte d'un système de recherche, si votre requête de recherche est «Qu'est-ce que le prétraitement de texte ?», vous voulez que le système de recherche se concentre sur les documents qui parlent de «prétraitement de texte» plutôt que sur les documents qui parlent de «Qu'est-ce que le». Cela peut être fait en empêchant tous les mots de votre liste de mots vides d'être analysés. Les mots vides sont couramment appliqués dans les systèmes de recherche, les applications de classification de texte, la modélisation de sujets, l'extraction de sujets et autres.\n",
    "\n",
    "Par contre, la suppression des mots vides, bien qu'efficace dans les systèmes de recherche et d'extraction de sujets, s'est avérée non critique dans les algorithmes de classification and ils sont assez importants dans la création d'un [**modèle de langage**](https://medium.com/@pierre_guillou/nlp-fastai-gpt-2-16ee145a4a28) dans lequel le sens d'un mot dépend de tous les mots environnants. Cependant, cela aide à réduire le nombre de fonctionnalités prises en compte, ce qui permet de garder vos modèles de taille décente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\") # charge le modèle en anglais\n",
    "\n",
    "spacy_stopwords = list(spacy.lang.en.stop_words.STOP_WORDS) # la liste des mots vides de SpaCy\n",
    "\n",
    "punctuation = list(string.punctuation) # une liste avec ponctuations\n",
    "\n",
    "spacy_stopwords[:10], punctuation[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spacy_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous créons une fonction pour prétraiter tous les documents dans le dataframe Pandas. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(phrase):\n",
    "\n",
    "    phrase = phrase.lower()\n",
    "\n",
    "    tokens = [token.text.replace('\\n', '').strip() for token in nlp(phrase) \n",
    "              if token.text.lower() not in spacy_stopwords + punctuation]\n",
    "    \n",
    "    tokens = [token for token in tokens if len(token) > 0]\n",
    "    \n",
    "    if len(tokens) > 0:\n",
    "        return ' '.join(tokens)\n",
    "    return None\n",
    "\n",
    "data['cleaned_text'] = data['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/Tobacco3482-text/Scientific/2051025161.txt</td>\n",
       "      <td>Scientific</td>\n",
       "      <td>MONTHLY SUMMARY\\n\\n25 JANUARY 1986-\\n24 FEBRUA...</td>\n",
       "      <td>monthly summary 25 january 1986- 24 february 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/Tobacco3482-text/Note/87705667.txt</td>\n",
       "      <td>Note</td>\n",
       "      <td>2/14/90\\n\\n   \\n\\nDATE:\\n\\n \\n\\n  \\n \\n\\nSUBJE...</td>\n",
       "      <td>2/14/90 date subject mifor information jplease...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/Tobacco3482-text/Scientific/50590463-0469...</td>\n",
       "      <td>Scientific</td>\n",
       "      <td>Cell, Vol. 59, 1107-1113, December 22, 1989, C...</td>\n",
       "      <td>cell vol 59 1107 1113 december 22 1989 copyrig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/Tobacco3482-text/Resume/50617225-7226.txt</td>\n",
       "      <td>Resume</td>\n",
       "      <td>BIOGRAPHICAS SKETCH\\n\\nGive the following info...</td>\n",
       "      <td>biographicas sketch following information key ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/Tobacco3482-text/News/2078115137.txt</td>\n",
       "      <td>News</td>\n",
       "      <td>Tae NEWS &amp; OBSERVER\\n\\nFaday, Ocroagr 15, 1999...</td>\n",
       "      <td>tae news observer faday ocroagr 15 1999 advise...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path       label  \\\n",
       "0    data/Tobacco3482-text/Scientific/2051025161.txt  Scientific   \n",
       "1            data/Tobacco3482-text/Note/87705667.txt        Note   \n",
       "2  data/Tobacco3482-text/Scientific/50590463-0469...  Scientific   \n",
       "3     data/Tobacco3482-text/Resume/50617225-7226.txt      Resume   \n",
       "4          data/Tobacco3482-text/News/2078115137.txt        News   \n",
       "\n",
       "                                                text  \\\n",
       "0  MONTHLY SUMMARY\\n\\n25 JANUARY 1986-\\n24 FEBRUA...   \n",
       "1  2/14/90\\n\\n   \\n\\nDATE:\\n\\n \\n\\n  \\n \\n\\nSUBJE...   \n",
       "2  Cell, Vol. 59, 1107-1113, December 22, 1989, C...   \n",
       "3  BIOGRAPHICAS SKETCH\\n\\nGive the following info...   \n",
       "4  Tae NEWS & OBSERVER\\n\\nFaday, Ocroagr 15, 1999...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  monthly summary 25 january 1986- 24 february 1...  \n",
       "1  2/14/90 date subject mifor information jplease...  \n",
       "2  cell vol 59 1107 1113 december 22 1989 copyrig...  \n",
       "3  biographicas sketch following information key ...  \n",
       "4  tae news observer faday ocroagr 15 1999 advise...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head() # visualisez les donnees apres pre-traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Avant:', data['text'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Après:', data['cleaned_text'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois que le prétraitement peut supprimer de nombreux mots, il est possible que les documents restent vides. Nous allons vérifier cela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a des documents sans mots restants, nous les rejetons donc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['cleaned_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifiez à nouveau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**<span style=\"color:red\">To do</span>**:\n",
    "\n",
    "> * Maintenant que nous avons utilisé [`spaCy`](https://spacy.io) pour le prétraitement, écrivez une fonction qui utilise la bibliothèque [`nltk`](https://www.nltk.org/). \n",
    "> * Utilisez [```word_tokenize```](https://www.nltk.org/api/nltk.tokenize.html) pour la tokenisation, téléchargez les mots vides nltk avec ``python -m nltk.downloader stopwords`` et supprimez tous les autres **tokens** qui, selon vous, nuiront à la précision des algorithmes d'apprentissage automatique (nombres? Ponctuation? Etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!python -m nltk.downloader stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "\n",
    "print(nltk_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_with_nltk(phrase):\n",
    "    \n",
    "    phrase = phrase.lower()\n",
    "    \n",
    "    tokens = # YOUR CODE HERE\n",
    "    \n",
    "    tokens = [token for token in tokens if len(token) > 0]\n",
    "    \n",
    "    if len(tokens) > 0:\n",
    "        return ' '.join(tokens)\n",
    "    return None\n",
    "\n",
    "data['cleaned_text_nltk'] = data['text'].apply(preprocess_with_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous savons **nettoyer les données**, nous pouvons aussi analyser le corpus avec des autres stratistiques, comme les occurences de chaque mot. Nous utilisons la classe [`nltk.FreqDist`](https://www.nltk.org/api/nltk.html?highlight=freqdist).\n",
    "\n",
    "Plus d'informations pour [`numpy hstack`](https://numpy.org/doc/stable/reference/generated/numpy.hstack.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "list_a = [1, 1, 1, 1, 1]\n",
    "list_b = [2, 2, 2, 2, 2]\n",
    "\n",
    "np.hstack([list_a, list_b]) # Exemple de fonctionnement de cette méthode avec des listes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "freq = nltk.FreqDist(np.hstack([text.split(' ') for text in data['cleaned_text']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in ['cigarettes', 'cancer', 'alcohol', 'death']:\n",
    "    print(word, ':', freq[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut grâce à `FreqDist` récupérer les 25 termes les plus courants du corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(freq.most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = freq.plot(50, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2  Lemmatisation\n",
    "\n",
    "Le processus de « [**lemmatisation**](https://fr.wikipedia.org/wiki/Lemmatisation) » consiste à représenter les mots (ou [lemmes](https://fr.wikipedia.org/wiki/Lemme_(linguistique)) en linguistique) sous leur forme canonique. Par exemple pour un verbe, ce sera son infinitif. Pour un nom, son masculin singulier. On ne **conserve que le sens des mots** utilisés dans le corpus. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Better to have friends and games during worse times \")\n",
    "\n",
    "for token in doc:\n",
    "    print('Word:', token.text, '\\t', 'Lemma:', token.lemma_) # Comment obtenir le lemme d'un mot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">To do</span>**:\n",
    "\n",
    "> * Ecrivez une méthode de prétraitement qui effectue également la lemmatisation. Utilisez [```spaCy```](https://spacy.io/usage/linguistic-features#lemmatization/) pour la tokenisation, et supprimez les mots vides et punctuation (`spacy_stopwords + punctuation`), et tous les autres **tokens** qui, selon vous, nuiront à la précision des algorithmes d'apprentissage automatique (nombres ? ponctuation ? Etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_lemmatisation(phrase):\n",
    "    \n",
    "    phrase = phrase.lower()\n",
    "\n",
    "    tokens = # YOUR CODE HERE\n",
    "    \n",
    "    tokens = [token for token in tokens if len(token) > 0]\n",
    "    \n",
    "    if len(tokens) > 0:\n",
    "        return ' '.join(tokens)\n",
    "    return None\n",
    "\n",
    "data['cleaned_text_with_lemmatisation'] = data['text'].apply(preprocess_with_lemmatisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['cleaned_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['cleaned_text_with_lemmatisation'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2.3 Racinisation\n",
    "\n",
    "Un autre processus, la [**racinisation**](https://fr.wikipedia.org/wiki/Racinisation) (ou *stemming* en anglais) a une approche similaire. Cela consiste à ne conserver que la racine des mots étudiés : on supprime suffixes, préfixes et autres afin de ne conserver que la racine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import EnglishStemmer\n",
    "stemmer = EnglishStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **racinisation** va tronquer les mots, par exemple comme ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Better to have friends and games during worse times\")\n",
    "for token in doc:\n",
    "    print('Word:', token.text, '\\t', 'Lemma:', stemmer.stem(token.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">To do</span>**:\n",
    "\n",
    "> * Ecrivez une méthode de prétraitement qui effectue également la racinisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_stemming(phrase):\n",
    "    \n",
    "    phrase = phrase.lower()\n",
    "    \n",
    "    tokens = # YOUR CODE HERE\n",
    "    \n",
    "    tokens = [token for token in tokens if len(token) > 0]\n",
    "    \n",
    "    if len(tokens) > 0:\n",
    "        return ' '.join(tokens)\n",
    "    return None\n",
    "\n",
    "\n",
    "data['cleaned_text_with_stemming'] = data['text'].apply(preprocess_with_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Representation des mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4.1 « *bag of words* » (sac de mots)\n",
    "\n",
    "L’**extraction d’information** est l’étape qui suit le **nettoyage** du corpus. Pour ce faire, il nous faut changer la représentation du texte pour qu’il puisse être utilisé par un modèle statistique, et pour pouvoir l’exploiter.\n",
    "\n",
    "Une représentation **bag-of-words** classique sera donc celle dans laquelle on représente chaque document par un vecteur de la taille du vocabulaire $|V|$. On utilisera la matrice composée de l’ensemble de ces $N$ documents qui forment le corpus comme entrée de nos algorithmes.\n",
    "\n",
    "Nous allons voir comment cela fonctionne et imprimer les mots les plus pertinents à l’aide de la classe [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) de `scikit-learn`.\n",
    "\n",
    "Ceci est juste un petit exemple avec les documents de 110 à 130 (problèmes de ressources mémoire).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(np.hstack(data['cleaned_text'][110:130]))\n",
    "\n",
    "X = vectorizer.transform(np.hstack(data['cleaned_text'][110:130])) # \n",
    "\n",
    "weights = np.asarray(X.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': vectorizer.get_feature_names(), 'weight': weights})\n",
    "weights_df.sort_values(by='weight', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 « *TF-IDF* » (*Term-Frequency* − *Inverse Document Frequency*)\n",
    "\n",
    "Ce que nous désirons faire avec [**TF-IDF**](https://fr.wikipedia.org/wiki/TF-IDF) est de mettre en application ce qui est décrit précédemment. Nous voulons pondérer la fréquence d’apparition d’un lemme dans un document par rapport à son apparition dans l’ensemble des documents du corpus. La fréquence de chaque lemme est donc pondérée à l’ensemble du corpus.\n",
    "\n",
    "En l’occurence, la métrique **TF-IDF** (*Term-Frequency* − *Inverse Document Frequency*) utilise comme indicateur de similarité l’*inverse document frequency* qui est l’inverse de la proportion de document qui contient le terme, à l'échelle logarithmique.\n",
    "\n",
    "Nous calculons donc le poids **TF-IDF** final attribué au n-gramme :\n",
    "\n",
    "$tfidf_{i,j} = tf_{i,j} × idf_{i}$\n",
    "\n",
    "Pour connaître les termes qui représentent le plus un document, nous allons utiliser la [fonction **TF-IDF**](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) de `scikit-learn`.\n",
    "\n",
    "\n",
    "**Note :** nous traitons des *n-grames*, donc utilisez le paramètre [`ngram_range`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) de la classe [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) lors de son instanciation.\n",
    "\n",
    "**Note :** pour transformer vos vecteurs résultats (obtenus par appel à `fit_transform`) en des données utilisables dans un `DataFrame`, vous devez utiliser `vectors.toarray()`. À la création de votre `DataFrame`, nommez les colonnes avec le nom des lemmes utilisés (ce sont les *features* de l’opération, vous pouvez les récupérer avec `get_feature_names()`).\n",
    "\n",
    "**Note :** comment on renverse une matrice ? En demandant sa transposée. Pour la transposée d’un `pd.DataFrame`, on accède simplement l’attribut `T` du `pd.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,10))\n",
    "vectors = vectorizer.fit_transform(np.hstack(data['cleaned_text']))#[110:130]\n",
    "\n",
    "weights = np.asarray(vectors.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': vectorizer.get_feature_names(), 'weight': weights})\n",
    "weights_df.sort_values(by='weight', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 2 Plongements de mots (Word Embeddings)\n",
    "\n",
    "La dernière répresentation des mots que nous analyserons est la répresentation **word embeddings**. \n",
    "Les **plongements de mots** (ou *word embedding*) est une technique d'apprentissage et de représentation de mots d’un texte. Elle consiste à représenter les mots sous forme de vecteurs de nombres réels qui ont la particularité d’être proches (dans leurs espaces vectoriels) si leurs contextes sont similaires.\n",
    "\n",
    "Ils peuvent être générés de plusieurs façons, comme les [réseaux de neuronnes](https://www.tensorflow.org/tutorials/text/word_embeddings) ou les matrices de [cooccurences](https://fr.wikipedia.org/wiki/Cooccurrence), des modèles probabilistes (TF-IDF vecteurs), etc.\n",
    "\n",
    "Cette nouvelle représentation a ceci de particulier que les mots apparaissant dans des contextes similaires possèdent des vecteurs correspondants qui sont relativement proches. Par exemple, on pourrait s'attendre à ce que les mots « chien » et « chat » soient représentés par des vecteurs relativement peu distants dans l'espace vectoriel où sont définis ces vecteurs. Cette technique est basée sur l'hypothèse qui veut que les mots apparaissant dans des contextes similaires ont des significations apparentées. \n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"images/we.png\" width=\"300\" height=\"400\">\n",
    "    <figcaption>Source: <a href=\"https://unbabel.com/blog/fr/lia-vous-parle-mais-comprend-elle-ce-quelle-dit/\"> link</a></figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "Différents  plongements (embeddings) de mots qui ont été entrainées sur de grands ensembles de données et disponibles :\n",
    "\n",
    "*  [Collobert & Weston](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf) https://ronan.collobert.com/senna/\n",
    "* [Word2Vec Google News](https://code.google.com/archive/p/word2vec/)\n",
    "* [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "* [FastText](https://github.com/facebookresearch/fastText)\n",
    "\n",
    "\n",
    "Le word embedding d'un mot peut être récupérée en utilisant la bibliothèque **spaCy** (cette bibliothèque utilise le modèle [GloVe](https://nlp.stanford.edu/projects/glove/)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = nlp(\"cigarretes\")\n",
    "token, token.vector.shape, token.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spaCy** est capable de comparer deux mots et de faire une prédiction de leur similitude. La prédiction de la similarité est utile pour créer des systèmes de recommandation ou pour signaler les doublons. Par exemple, vous pouvez suggérer un contenu similaire à ce que l'utilisateur recherche actuellement ou étiqueter un ticket d'assistance comme doublon s'il est très similaire à un ticket déjà existant.\n",
    "\n",
    "Chaque **Doc**, **Span** et **Token** est livré avec une **.similarity()** méthode qui vous permet de le comparer avec un autre objet et de déterminer la similitude. Bien sûr, la similitude est toujours subjective - si «chien» et «chat» sont similaires dépend vraiment de la façon dont vous le regardez. Le modèle de similarité de **spaCy** suppose généralement une définition assez générale de la similitude.\n",
    "\n",
    "Généralement, ene fois que nous aurons des vecteurs du texte donné, pour calculer la similitude entre les vecteurs générés, des méthodes statistiques pour la similitude vectorielle peuvent être utilisées. Ces techniques sont la *similitude cosinus*, la *distance euclidienne*, la *distance de Jaccard*, la distance *word mover*. La *similitude cosinus* est la technique qui est largement utilisée pour la similitude de texte (**spaCy** utilise cette similitude).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = nlp('cigarettes')\n",
    "for word in ['cigarettes', 'health', 'cancer', 'capitalism', 'doctor', 'nurse', 'hospital', 'money', 'death']:\n",
    "    word = nlp(word)\n",
    "    print('cigarettes', '~', word, ':', token.similarity(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour voir l'exemple de la figure présentée précédemment, nous calculons la distance entre tous les mots * spaCy * pour trouver les mots les plus similaires pour le calcul: **king − man + woman ≈ queen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    " \n",
    "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
    " \n",
    "man = nlp.vocab['man'].vector\n",
    "woman = nlp.vocab['woman'].vector\n",
    "queen = nlp.vocab['queen'].vector\n",
    "king = nlp.vocab['king'].vector\n",
    " \n",
    "# Nous devons maintenant trouver le vecteur le plus proche du vocabulaire du résultat de \"man\" - \"woman\" + \"queen\"\n",
    "maybe_king = man - woman + queen\n",
    "computed_similarities = []\n",
    " \n",
    "for word in nlp.vocab:\n",
    "    # Ignorer les mots sans vecteurs\n",
    "    if not word.has_vector:\n",
    "        continue\n",
    "\n",
    "    similarity = cosine_similarity(maybe_king, word.vector)\n",
    "    computed_similarities.append((word, similarity))\n",
    "\n",
    "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
    "print('man - woman + queen=', [w[0].text for w in computed_similarities[:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Preparation pour apprentissage automatique (machine learning) et apprentissage profond (deep learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Diviser les jeux de données \n",
    "Afin d'entraîner des modèles pour apprentissage automatique et évaluer la performance de ses modèles avec chaque répresentation de mots, nous allons diviser les jeux de données en : entraînement, validation et test.\n",
    "\n",
    "Afin d'évaluer correctement la performance de chaque modèle, il est très important que les données d'entraînement et de test soient différentes. La bibliothèque <b>sklearn</b> a la fonction <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\"><i>train_test_split</i></a> qui divise un jeux de données. Le parametre <i>test_size</i> défini la taille du test (pourcentage) dans les jeux de données.\n",
    "\n",
    "Pour comprendre pourquoi nous devons diviser les données de cette manière, vous pouvez regarder Andrew Ng expliquer [ici](https://www.youtube.com/watch?v=1waHlpKiNyY). Ng est professeur à l'Université de Stanford et pionnier de l'éducation en ligne, Ng a cofondé [Coursera](https://www.coursera.org/learn/machine-learning) et [deeplearning.ai](https://www.deeplearning.ai/). Il a mené avec succès de nombreux efforts pour «démocratiser l'apprentissage profond» en enseignant plus de 2,5 millions d'étudiants grâce à ses cours en ligne. Il est l'un des informaticiens les plus connus et les plus influents au monde.\n",
    "\n",
    "Dans notre cas, nous irons fournir les phrases et les classes pour chaque phrase et la taille du jeu de données de test. Pour l'instant, on a pas besoin de donees de validation. On va voir de ca plus tard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour plus de simplicité, nous allons travailler uniquement avec des données pour entraînement et test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Nous utilisons ici la colonne qui n'a pas été prétraitée.\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.1)\n",
    "print('Train:', len(X_train), 'texts', 'Test:', len(X_test), 'texts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Apprentissage automatique (machine learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de la catégorisation de textes est de pouvoir associer automatiquement des documents à des classes (catégories, étiquettes, index) prédéfinies. Nous nous plaçons dans le cadre de l'apprentissage automatique supervisé. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Approaches de classification\n",
    "\n",
    "La classification naïve bayésienne est un type de classification bayésienne probabiliste simple basée sur le théorème de Bayes avec une forte indépendance des hypothèses. Elle met en œuvre un classifieur bayésien naïf, ou classifieur naïf de Bayes, appartenant à la famille des classifieurs linéaires. \n",
    "\n",
    "Un classifieur bayésien naïf suppose que l'existence d'une caractéristique pour une classe, est indépendante de l'existence d'autres caractéristiques. Un fruit peut être considéré comme une pomme s'il est rouge, arrondi, et fait une dizaine de centimètres. Même si ces caractéristiques sont liées dans la réalité, un classifieur bayésien naïf déterminera que le fruit est une pomme en considérant indépendamment ces caractéristiques de couleur, de forme et de taille. \n",
    "\n",
    "Le modèle probabiliste de cette approache est le modèle conditionnel $P(X | x_1,...,x_n)$, où $X$ est la variable « de classe » (celle qui indique si un individu appartient à une classe donnée) conditionnée par plusieurs variables caractéristiques $x_i$ (par exemple l’aile, le bec et cancane).\n",
    "\n",
    "Le théorème de Bayes s’énonce avec nos notations de la manière suivante :\n",
    "\n",
    "<center>\n",
    "$P(X|x_1, ..., x_n) = \\frac{P(x_1, ..., x_n | X)P(X)}{P(x_1, ..., x_n)}$\n",
    "</center>\n",
    "\n",
    "L'avantage du classifieur bayésien naïf est qu'il requiert relativement peu de données d'entraînement pour estimer les paramètres nécessaires à la classification, à savoir moyennes et variances des différentes variables.\n",
    "\n",
    "La bibliothèque sklearn fournit une classe [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) qui nous permet d'entraîner et tester un modèle à partir d'un jeux de données d'entraînement.\n",
    "\n",
    "Tout d'abord, nous représentons les documents en utilisant TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train) # Nous laissons le TF-IDF voir les données d'entraînement\n",
    "\n",
    "X_test_tfidf = vectorizer.transform(X_test) # Nous transformons également les données de test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous utilisons les données d'entraînement pour entraîner le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "_ = clf.fit(X_train_tfidf, y_train) # Nous entrainons le modèle avec la méthode `fit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir entraîné notre modèle, nous pouvons faire la prediction des classes des messages du jeux de données de test.\n",
    " \n",
    "## Analyse d'erreurs\n",
    "\n",
    "Le taux d'erreur de classification donne une évaluation des performances pour toutes les classes. Mais comme les classes ne sont pas également réparties, elles peuvent ne pas être également bien modélisées. Afin d'avoir une meilleure idée des performances du classifieur, des métriques détaillées doivent être utilisées:\n",
    "\n",
    "* [metrics.classification_report](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) fournit une analyse détaillée par classe: la précision (parmi tous les exemples classés en classe X, combien sont réellement de la classeX) et le rappel (parmi tous les exemples qui sont de la classe X, combien sont classés en classe X) et le F-Score qui est une moyenne harmonique pondérée de la précision et du rappel.\n",
    "* [metrics.confusion_matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) qui donnent les confusions entre les classes.\n",
    "\n",
    "F1-score = 2 x $\\frac{précision\\ x\\ rappel}{précision\\ +\\ rappel}$\n",
    "\n",
    " - précision : la proportion d'identifications positives était effectivement correcte.\n",
    " - rappel : la proportion de résultats positifs réels a été identifiée correctement. \n",
    "\n",
    "\n",
    "**<span style=\"color:red\">To do</span>**:\n",
    "\n",
    "> * Rapportez le `classification_report` pour votre classificateur. Quelles classes ont les meilleurs scores ? Pourquoi ?\n",
    "> * Rapportez la `confusion_matrix` pour votre classificateur. Quelles classes sont les plus confuses ? Pourquoi ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "plt.rcParams.update({'font.size': 9})\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(clf, X_test_tfidf, y_test)  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les machines à vecteurs de support (en anglais support vector machine, SVM) sont un ensemble de techniques d'apprentissage supervisé destinées à résoudre des problèmes de discrimination et de régression. Ces techniques reposent sur deux idées clés : la notion de marge maximale et la notion de fonction noyau.\n",
    "\n",
    "La <b>marge</b> est la distance entre la frontière de séparation et les échantillons les plus proches. Dans les SVM, la frontière de séparation est choisie comme celle qui maximise la marge.\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"images/marge.png\" width=\"400\" height=\"400\">\n",
    "    <figcaption>Source: <a href=\"https://fr.wikipedia.org/wiki/Machine_%C3%A0_vecteurs_de_support\"> link</a></figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "La deuxième idée clé des SVM est de transformer l'espace de représentation des données d'entrées en un espace de plus grande dimension, dans lequel il est probable qu'il existe une séparation linéaire. Les fonctions <b>noyau</b> permettent de transformer un produit scalaire dans un espace de grande dimension, ce qui est coûteux, en une simple évaluation ponctuelle d'une fonction. Des noyaux usuels employés avec les SVM sont : le noyau polynomial et le noyau gaussien.\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"images/noyau.png\" width=\"400\" height=\"400\">\n",
    "    <figcaption>Source: <a href=\"https://fr.wikipedia.org/wiki/Machine_%C3%A0_vecteurs_de_support\"> link</a></figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "La bibliothèque sklearn propose un module pour utiliser de [SVMs](https://scikit-learn.org/stable/modules/svm.html). Si vous souhaitez utiliser SVM avec d'autres noyaux, utilisez [svm.SVC()](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">To do</span>**:\n",
    "\n",
    "> * Entraînez, testez et faites l'évaluation du modèle SVM linéaire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">To do</span>**:\n",
    "\n",
    "> * Utilisez les autres méthodes de prétraitement (avec lemmatisation, avec racinisation, etc. `cleaned_text`, `cleaned_text_with_lemmatisation`, `cleaned_text_with_stemming`) et comparez les résultats de ces trois modèles. Vous devez réécrire le code à partir de \"5.1 Diviser les jeux de données\" et changer la colonne de données.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Apprentissage profond (deep learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification avec des réseaux de neurones\n",
    "\n",
    "Les réseaux de neurones peuvent être entraînés pour apprendre à la fois la représentation vectorielle des mots (au lieu de tf-idf) et comment classer les documents. Le code ci-dessous vous permet d'entraîner un classificateur de texte neuronal à l'aide de l'intégration de mots à l'aide de Keras. La plupart du code est écrit, il suffit de définir l'architecture du réseau avec les bons paramètres avant de l'entraîner:\n",
    "\n",
    "**<span style=\"color:red\">To do</span>**:\n",
    "\n",
    "> * Aller plus loin. Vérifiez [Text classification Keras examples](https://keras.io/examples/nlp/)\n",
    "\n",
    "> * Définissez un réseau de neurones dans la fonction `get_model()` avec les paramètres suivants:\n",
    "> * n'utiliser que les 20000 mots les plus fréquents dans les documents MAX_FEATURES\n",
    "> * utiliser 1024 comme nombre maximal de mots dans les articles MAX_TEXT_LENGTH\n",
    "> * utiliser une taille de 300 pour les embeddings EMBEDDING_SIZE: [word embeddings](https://keras.io/layers/embeddings/)\n",
    "> * utiliser d'autres valeurs pour les ngram_filters filtres convolutifs: [couche convolutive 1D](https://keras.io/layers/convolutional/#conv1d)\n",
    "> * Ajoutez une couche Dropout à l'endroit indiqué avec le valeur 0,2 [Dropout](https://keras.io/api/layers/regularization_layers/dropout/)\n",
    "> * Former le modèle.\n",
    "\n",
    "> * Comment ce réseau de neurones se compare-t-il aux autres modèles? \n",
    "> * Quelle est la performance?\n",
    "> * Qu'apporte le changement de paramètres dans la performance?\n",
    "> * Utilisez des plongements pré-entraînés et chargez-les en tant que poids dans ce modèle (au lieu de ceux générés aléatoirement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "from tensorflow.keras.layers import Dense, Embedding, Input\n",
    "from tensorflow.keras.layers import GRU, Dropout, MaxPooling1D, Conv1D, Flatten, GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import (classification_report, \n",
    "                             precision_recall_fscore_support, \n",
    "                             accuracy_score)\n",
    "\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurez les paramètres du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 20000 # YOUR CODE HERE\n",
    "MAX_TEXT_LENGTH = 1024 # YOUR CODE HERE\n",
    "EMBEDDING_SIZE  = 300 # YOUR CODE HERE\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "VALIDATION_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette methode transforme chaque texte des textes en une séquence d'entiers [Plus d'info](https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do). Donc, il prend chaque mot dans le texte et le remplace par sa valeur entière correspondante du dictionnaire [word_index](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer). Le dictionnaire a été obtenu à partir de [fit_on_texts](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(train_raw_text, test_raw_text):\n",
    "    \n",
    "    tokenizer = text.Tokenizer(num_words=MAX_FEATURES)\n",
    "\n",
    "    tokenizer.fit_on_texts(list(train_raw_text))\n",
    "    \n",
    "    train_tokenized = tokenizer.texts_to_sequences(train_raw_text)\n",
    "    test_tokenized = tokenizer.texts_to_sequences(test_raw_text)\n",
    "    \n",
    "    return sequence.pad_sequences(train_tokenized, maxlen=MAX_TEXT_LENGTH), \\\n",
    "           sequence.pad_sequences(test_tokenized, maxlen=MAX_TEXT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d4f76ff933b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Obtenez la liste des différentes classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mCLASSES_LIST\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCLASSES_LIST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Obtenez la liste des différentes classes\n",
    "\n",
    "CLASSES_LIST = np.unique(data['label'])\n",
    "n_out = len(CLASSES_LIST)\n",
    "\n",
    "print(CLASSES_LIST, n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "\n",
    "    inputs = Input(shape=(MAX_TEXT_LENGTH,))\n",
    "    model = Embedding(MAX_FEATURES, EMBEDDING_SIZE)(inputs)\n",
    "    # YOUR CODE HERE couche Dropout\n",
    "    \n",
    "    ngram_filters = [1, 2, 3] # YOUR CODE HERE Modifier les valeurs n-gramme\n",
    "    filters = [300]*3\n",
    "\n",
    "    convs = []\n",
    "    for kernel_size, filter_length in zip(ngram_filters, filters):\n",
    "        conv = Conv1D(filters=filter_length,\n",
    "                      kernel_size=kernel_size,\n",
    "                      padding='same',\n",
    "                      activation='relu')(model)\n",
    "        convs.append(conv)\n",
    "\n",
    "    model = Concatenate()(convs)\n",
    "    model = GlobalMaxPooling1D()(model)    \n",
    "\n",
    "    outputs = Dense(n_out, activation=\"softmax\")(model)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette méthode est pour entraîner et tester le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fit_predict(model, X_train, X_test, y):\n",
    "    \n",
    "    model.fit(X_train, y,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=EPOCHS, verbose=1,\n",
    "              validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "    return model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour que le modèle comprenne les étiquettes, elles doivent être sous forme entière. Ainsi, pour cela, il existe cette méthode: [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CLASSES_LIST' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-706692dd94a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabel_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_en\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCLASSES_LIST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my_train_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CLASSES_LIST' is not defined"
     ]
    }
   ],
   "source": [
    "# Convertir la chaîne de classe en index (entiers)\n",
    "from sklearn import preprocessing\n",
    "label_en = preprocessing.LabelEncoder()\n",
    "le=label_en\n",
    "le.fit(CLASSES_LIST)\n",
    "\n",
    "y_train_encoded = le.transform(y_train) \n",
    "y_test_encoded = le.transform(y_test) \n",
    "train_y_cat = to_categorical(y_train_encoded, n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 2, 1, 2])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(622, 70)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtenir les données textuelles dans le format correct pour le réseau neuronal\n",
    "x_vec_train, x_vec_test = get_train_test(X_train, X_test)\n",
    "len(x_vec_train), len(x_vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définir la topologie du réseau neuronal\n",
    "model = get_model()\n",
    "\n",
    "# Train and predict\n",
    "y_predicted = train_fit_predict(model, x_vec_train, x_vec_test, train_y_cat).argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_encoded, y_predicted, target_names=CLASSES_LIST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "et voilà !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
