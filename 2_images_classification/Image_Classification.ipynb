{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification des images\n",
    "                                            Emanuela Boros\n",
    "                                            Université de La Rochelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "La classification des objets consiste à attribuer une classe à un objet. \n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"images/classification.jpeg\" width=\"400\" height=\"600\">\n",
    "    <figcaption>Source: <a href=\"https://www.freecodecamp.org/news/chihuahua-or-muffin-my-search-for-the-best-computer-vision-api-cbda4d6b425d/\">Link</a></figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "Dans ce cours, nous explorerons un ensemble de données, une analyse de texte et plusieurs approches basées sur l'apprentissage automatique et profond, de la manière suivante:\n",
    "\n",
    "- **Récupération** du corpus d'images (*.csv, *.json, etc.)\n",
    "- **Prétraitement** des images (image pre-processing) : réduction de taille etc\n",
    "- **Representation** des images (pixels, HOG, SIFT)\n",
    "- **Apprentissage automatique** (machine learning) et **apprentissage profond** (deep learning)\n",
    "- **Analyse d'erreurs** (evaluation, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Récupération, Prétraitement et Exploration du corpus des images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 1. Contexte\n",
    "\n",
    "Le gouvernement américain a attaqué en justice cinq grands groupes américains du tabac pour avoir amassé d'importants bénéfices en mentant sur les dangers de la cigarette. Le cigarettiers  se sont entendus dès 1953, pour \"mener ensemble une vaste campagne de relations publiques afin de contrer les preuves de plus en plus manifestes d'un lien entre la consommation de tabac et des maladies graves\". \n",
    "\n",
    "Dans ce procès 6,910,192 de documents ont été collectés et numérisés. Afin de faciliter l'exploitation de ces documents par les avocats, vous êtes en charge de mettre en place une classification automatique des types de documents: **Advertisement, Email, Form, Letter, Memo, News, Note, Report, Resume, Scientific**.\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>Letter</td>\n",
    "     <td>News</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"images/50661869-1869.jpg\" width=270 height=480></td>\n",
    "    <td><img src=\"images/10031617.jpg\" width=270 height=480></td>\n",
    "  </tr>\n",
    " </table>\n",
    "\n",
    "\n",
    "Un échantillon aléatoire des documents a été collecté et des opérateurs ont classé les documents dans des répertoires correspondant aux classes de documents : lettres, rapports, notes, email, etc. Vous avez à votre disposition : \n",
    "\n",
    "- le chemin de l'image; <ins>**path**</ins>\n",
    "- les classes des documents définies par des opérateurs; <ins>**label**</ins>\n",
    "\n",
    "Nous pourrons charger et regarder le structure de ces commentaires avec la bibliothèque pandas (plus d'info [```pandas```](https://pandas.pydata.org/docs/user_guide/index.html)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # https://pandas.pydata.org/docs/\n",
    "\n",
    "data = pd.read_csv('data/Tobacco3482-images/tobacco_images.csv')#.sample(frac=.5) # lire le fichier .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head() # afficher les 5 premières lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail() # afficher les 5 dernières lignes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec Pandas, nous pouvons commencer avec quelques statistiques simples. Les résultats suivants seront dans l'ordre décroissant de sorte que le premier élément soit l'élément le plus fréquent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec Pandas, nous pouvons également visualiser ces statistiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = data.label.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons vu que le fichier .csv contenait les chemins et les étiquettes des documents. Maintenant, nous devons lire les fichiers à partir des chemins spécifiés pour obtenir les images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "images = []\n",
    "for idx, line in data.iterrows(): # itérer dans un dataframe Pandas\n",
    "    image = cv2.imread(line['path'])\n",
    "    images.append(image)\n",
    "    \n",
    "data['image'] = images # créer une nouvelle colonne dans le dataframe Pandas avec les textes associés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Pré-traitement des images \n",
    "\n",
    "Comme dans la première partie où nous avons représenté les mots par des vecteurs, nous allons représenter les images par des vecteurs aussi. Comme pour les mots, il y a plusieurs types de représentations pour les images. Chaque représentation peut ajouter des informations supplémentaires sur une image. Plus précisément, certains types de représentations fournissent des informations qui ne sont pas évidentes dans une représentation triviale.\n",
    "\n",
    "Parmi les représentations existantes, nous allons analyser:\n",
    " - RGB\n",
    " - Gris\n",
    " - HOG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB (Red, Green, Blue)\n",
    "\n",
    "Rouge, vert, bleu, abrégé en RVB ou en RGB (de l'anglais « red, green, blue ») est un système de codage informatique des couleurs, le plus proche du matériel. Le codage RVB indique une valeur pour chacune de ces couleurs primaires. \n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "    <img src=\"images/sample_image.png\" width=\"400\" height=\"600\">\n",
    "    <figcaption>height, width, channels</figcaption>\n",
    "    </center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('height, width, channels', data['image'][0].shape) # shape = height, width, channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(data['image'][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puisque nos données sont grises, nous prenons une autre image comme exemple (pour afficher les valeurs RGB):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "sample_image = cv2.imread('images/apples.jpg', cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def get_RGB_image(image):\n",
    "    # converting image to HSV format with cv2.COLOR_BGR2RGB\n",
    "    sample_rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return sample_rgb_image\n",
    "\n",
    "sample_image = get_RGB_image(sample_image)\n",
    "\n",
    "print(\"Image Properties\")\n",
    "print(\"Number of Pixels: \" + str(sample_image.size))\n",
    "print(\"Shape/Dimensions (height, width, channels): \" + str(sample_image.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pixels:\", sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(sample_image)\n",
    "plt.title('Original Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.resize(sample_image, (300, 300))\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title('Resized Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We separate the image in the 3 channels: red, green and blue\n",
    "channel_initials = list('RGB')\n",
    "\n",
    "for channel_index in range(3):\n",
    "    \n",
    "    channel = np.zeros(shape=sample_image.shape, dtype='uint8')\n",
    "    channel[:,:,channel_index] = sample_image[:,:,channel_index]\n",
    "\n",
    "    plt.imshow(channel)\n",
    "    plt.title(channel_initials[channel_index] + ' channel')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gris\n",
    "\n",
    "Une autre possibilité est la représentation de ces images dans les nuances de gris. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gray_image(image):\n",
    "\n",
    "    # converting image to HSV format with cv2.COLOR_BGR2GRAY\n",
    "    sample_gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return sample_gray_image\n",
    "\n",
    "sample_gray_image = get_gray_image(sample_image)\n",
    "plt.imshow(sample_gray_image, cmap='gray', vmin=0, vmax=255)\n",
    "plt.title('Gray Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HSV (hue, saturation, value)\n",
    "\n",
    "HSV est une représentation alternative du modèle de couleur RGB. La représentation HSV modélise la façon dont les couleurs apparaissent sous la lumière.\n",
    "Teinte (hue) spécifie l'angle de la couleur sur le cercle de couleur RGB.\n",
    "La saturation contrôle la quantité de couleur utilisée.\n",
    "La valeur (value) contrôle la luminosité de la couleur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_HSV_image(image):\n",
    "    # converting image to HSV format with cv2.COLOR_BGR2HSV\n",
    "    sample_hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    return sample_hsv_image\n",
    "\n",
    "sample_hsv_image = get_gray_image(sample_image)\n",
    "plt.imshow(sample_hsv_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pixels:\", sample_hsv_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons également transformer l'image en **différentes nuances de gris**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gray_thresh_binary(image):\n",
    "    _, thresh_binary = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "    return thresh_binary\n",
    "\n",
    "def get_gray_thresh_binary_inv(image):\n",
    "    _, thresh_binary_inv = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "    return thresh_binary_inv\n",
    " \n",
    "def get_gray_thresh_trunc(image):\n",
    "    _, thresh_trunc = cv2.threshold(image, 127, 255, cv2.THRESH_TRUNC)\n",
    "    return thresh_trunc\n",
    " \n",
    "def get_gray_thresh_tozero(image):\n",
    "    _, thresh_tozero = cv2.threshold(image, 127, 255, cv2.THRESH_TOZERO)\n",
    "    return thresh_tozero\n",
    " \n",
    "def get_gray_thresh_tozero_inv(image):\n",
    "    _, thresh_tozero_inv = cv2.threshold(image, 127, 255, cv2.THRESH_TOZERO_INV)\n",
    "    return thresh_tozero_inv\n",
    "\n",
    "# Displaying the different thresholding styles\n",
    " \n",
    "names = ['Original Image','BINARY','THRESH_BINARY','THRESH_TRUNC','THRESH_TOZERO','THRESH_TOZERO_INV']\n",
    " \n",
    "images = [sample_gray_image, \n",
    "          get_gray_thresh_binary(sample_gray_image), \n",
    "          get_gray_thresh_binary_inv(sample_gray_image), \n",
    "          get_gray_thresh_trunc(sample_gray_image), \n",
    "          get_gray_thresh_tozero(sample_gray_image), \n",
    "          get_gray_thresh_tozero_inv(sample_gray_image)]\n",
    "\n",
    "for i in range(6):\n",
    " \n",
    "    plt.subplot(2, 3, i+1), plt.imshow(images[i], cmap='gray', vmin=0, vmax=255)\n",
    " \n",
    "    plt.title(names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Répresentation des images \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canny Edge Detection (détection de bord Canny) \n",
    "\n",
    "- est un algorithme de détection de bord populaire. Il a été développé par John F. Canny en 1986. C'est un algorithme en plusieurs étapes et nous passerons par plusieurs étapes. Deux des plus importants sont: la réduction du bruit (étant donné que la détection des contours est sensible au bruit dans l'image, la première étape consiste à supprimer le bruit dans l'image avec un filtre gaussien 5x5.) Et la recherche du gradient d'intensité de l'image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges(image):\n",
    "    #calculate the edges using Canny edge algorithm\n",
    "    edges_of_image = cv2.Canny(sample_gray_image, 100, 200) \n",
    "    return edges_of_image\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.imshow(sample_gray_image, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "#plot the edges\n",
    "edges_of_image = get_edges(image)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(edges_of_image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian blur : réduire le bruit de l'image\n",
    "\n",
    "Dans le traitement d'image, un flou gaussien (également connu sous le nom de lissage gaussien) est le résultat du flou d'une image par une fonction gaussienne (du nom du mathématicien et scientifique Carl Friedrich Gauss).\n",
    "\n",
    "C'est un effet largement utilisé généralement pour réduire le bruit de l'image et réduire les détails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(sample_gray_image, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "def get_smoothen_image(image):\n",
    "    #using the averaging kernel for image smoothening \n",
    "    averaging_kernel = np.ones((5,5), np.float32)/25\n",
    "    filtered_image = cv2.filter2D(image, -1, averaging_kernel) \n",
    "    return filtered_image\n",
    "\n",
    "#show smoothen image\n",
    "filtered_image = get_smoothen_image(sample_gray_image)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(filtered_image, cmap='gray') \n",
    "plt.show()\n",
    "\n",
    "def get_gaussian_blur(image):\n",
    "    #get a one dimensional Gaussian Kernel \n",
    "    gaussian_kernel_x = cv2.getGaussianKernel(15, 5) \n",
    "    gaussian_kernel_y = cv2.getGaussianKernel(15, 5) \n",
    "\n",
    "    #converting to two dimensional kernel using matrix multiplication \n",
    "    gaussian_kernel = gaussian_kernel_x * gaussian_kernel_y.T \n",
    "\n",
    "    #you can also use cv2.GaussianBLurring(image,(shape of kernel),standard deviation) instead of cv2.filter2D \n",
    "    filtered_image = cv2.filter2D(sample_gray_image, -1, gaussian_kernel) \n",
    "    return filtered_image\n",
    "\n",
    "\n",
    "#show smoothen image\n",
    "filtered_image = gaussian_blur(sample_gray_image)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(filtered_image, cmap='gray') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIFT (scale-invariant feature transform)\n",
    "\n",
    "La transformation de caractéristiques invariantes d'échelle (SIFT) est un algorithme de détection de caractéristiques en vision par ordinateur pour détecter et décrire les caractéristiques locales dans les images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#create sift object\n",
    "sift  = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "def get_SIFT_features(image):\n",
    "    #calculate keypoints and their orientation\n",
    "    keypoints, sift_image = sift.detectAndCompute(image, None)\n",
    "    return sift_image\n",
    "\n",
    "\n",
    "keypoints, sift_image = sift.detectAndCompute(sample_gray_image, None)\n",
    "#plot keypoints on the image\n",
    "image_with_keypoints = cv2.drawKeypoints(sample_gray_image, keypoints, sample_gray_image)\n",
    "\n",
    "#plot the image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image_with_keypoints)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of Oriented Gradients (HOG)\n",
    "\n",
    "Un histogramme de gradient orienté (HOG) est une caractéristique utilisée en vision par ordinateur pour la détection d'objet. La technique calcule des histogrammes locaux de l'orientation du gradient sur une grille dense, c'est-à-dire sur des zones régulièrement réparties sur l'image. Cette méthode est particulièrement efficace pour la détection de personnes. Plus d'info <a href=\"https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html\">ici</a>. Nous allons utiliser la bibliothèque <a href=\"https://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_rgb_to_gray.html\">skimage</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "\n",
    "def get_HOG_image(image):\n",
    "    # Get HOG features\n",
    "    hog_features, hog_image = hog(image,\n",
    "                                  visualize=True,\n",
    "                                  block_norm='L2-Hys',\n",
    "                                  pixels_per_cell=(16, 16))\n",
    "    return hog_image\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(sample_gray_image, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "hog_image = get_HOG_image(sample_gray_image)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(hog_image, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensuite, nous créons une fonction pour prétraiter tous les images dans le dataframe Pandas.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['image'][0].shape)\n",
    "print(data['image'][0].flatten().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    \n",
    "    # resize image\n",
    "    image = cv2.resize(image, (300, 300))\n",
    "    \n",
    "    # tranform image into grayscale\n",
    "    image = get_gray_image(image)\n",
    "    \n",
    "    # we need to work with vectors so we transform the 3D matrix (height, width, channels) in 1D (a list) \n",
    "    image = image.flatten()\n",
    "    \n",
    "    return image\n",
    "\n",
    "data['preprocessed_image'] = data['image'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head() # visualisez les donnees apres pre-traitement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparation pour apprentissage automatique (machine learning) et apprentissage profond (deep learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Diviser les jeux de données \n",
    "Afin d'entraîner des modèles pour apprentissage automatique et évaluer la performance de ses modèles avec chaque répresentation de mots, nous allons diviser les jeux de données en : entraînement, validation et test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour plus de simplicité, nous allons travailler uniquement avec des données pour entraînement et test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Nous utilisons ici la colonne qui a été prétraitée.\n",
    "X_train, X_test, y_train, y_test = train_test_split(list(data['preprocessed_image']), data['label'], test_size=0.1)\n",
    "print('Train:', len(X_train), 'images', 'Test:', len(X_test), 'images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Apprentissage automatique (machine learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif de la catégorisation de textes est de pouvoir associer automatiquement des documents à des classes (catégories, étiquettes, index) prédéfinies. Nous nous plaçons dans le cadre de l'apprentissage automatique supervisé. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Principal component analysis (PCA)\n",
    "\n",
    "L'analyse en composantes principales (ACP ou PCA en anglais pour principal component analysis) est une méthode de la famille de l'analyse des données et plus généralement de la statistique multivariée, qui consiste à transformer des variables liées entre elles en nouvelles variables décorrélées les unes des autres. \n",
    "\n",
    "Ces nouvelles variables sont nommées « composantes principales », ou axes principaux. Elle réduit le nombre de variables et rend l'information moins redondante sur une donnée. \n",
    "\n",
    "On utilise PCA pour extraire les composants les plus pertinents dans les vecteurs des images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# La normalisation des ensembles de données est une exigence \n",
    "# courante pour de nombreux estimateurs d'apprentissage automatique\n",
    "# Avant d'appliquer PCA, il est nécessaire de standardiser vos données \n",
    "# (travailler avec des images signifie également travailler avec de grandes valeurs).\n",
    "# Les algorithmes peuvent se comporter mal si les caractéristiques individuelles \n",
    "# ne ressemblent pas plus ou moins à des données standard normalement distribuées. \n",
    "# N'oubliez pas de normaliser vos données lorsque vous travaillez avec des images.\n",
    "standard_scaler = StandardScaler()\n",
    "X_train = standard_scaler.fit_transform(X_train)\n",
    "X_test = standard_scaler.transform(X_test)\n",
    "\n",
    "# Now, we apply PCA to reduce the vectors size\n",
    "pca = PCA(n_components=50)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "print('PCA train data matrix shape is: ', X_train.shape)\n",
    "print('PCA test data matrix shape is: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Approaches de classification\n",
    "\n",
    "### La régression logistique \n",
    "- est un algorithme de classification qui transforme sa sortie à l'aide de la fonction sigmoïde logistique pour donner une valeur de probabilité pour les classes de sortie.\n",
    "\n",
    "\n",
    "La bibliothèque sklearn fournit une classe [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) qui nous permet d'entraîner et tester un modèle à partir d'un jeux de données d'entraînement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, nous utilisons les données d'entraînement pour entraîner le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# all parameters not specified are set to their defaults\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "_ = classifier.fit(list(X_train), y_train) # Nous entrainons le modèle avec la méthode `fit`\n",
    "print('Training finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir entraîné notre modèle, nous pouvons faire la prediction des classes des messages du jeux de données de test.\n",
    " \n",
    "## Analyse d'erreurs\n",
    "\n",
    "Le taux d'erreur de classification donne une évaluation des performances pour toutes les classes. Mais comme les classes ne sont pas également réparties, elles peuvent ne pas être également bien modélisées. Afin d'avoir une meilleure idée des performances du classifieur, des métriques détaillées doivent être utilisées:\n",
    "\n",
    "* [metrics.classification_report](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) fournit une analyse détaillée par classe: la précision (parmi tous les exemples classés en classe X, combien sont réellement de la classeX) et le rappel (parmi tous les exemples qui sont de la classe X, combien sont classés en classe X) et le F-Score qui est une moyenne harmonique pondérée de la précision et du rappel.\n",
    "\n",
    "F1-score = 2 x $\\frac{précision\\ x\\ rappel}{précision\\ +\\ rappel}$\n",
    "\n",
    " - précision : la proportion d'identifications positives était effectivement correcte.\n",
    " - rappel : la proportion de résultats positifs réels a été identifiée correctement. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = classifier.predict(list(X_test))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True)\n",
    "plt.ylabel('True classes')\n",
    "plt.xlabel('Predicted classes')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "Les machines à vecteurs de support (en anglais support vector machine, SVM) sont un ensemble de techniques d'apprentissage supervisé destinées à résoudre des problèmes de discrimination et de régression. Ces techniques reposent sur deux idées clés : la notion de marge maximale et la notion de fonction noyau.\n",
    "\n",
    "La bibliothèque sklearn propose un module pour utiliser de [svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">To do</span>**:\n",
    "\n",
    "> * Entraînez, testez et faites l'évaluation du modèle SVM linéaire. Essayez également le SVM avec différents noyaux (kernel) : `poly`, `rbf`, `sigmoid`. Pour le SVM linéaire, c'est `SVC(kernel='linear')`.\n",
    "> * Rapportez le `classification_report` pour votre classificateur. Quelles classes ont les meilleurs scores ? Pourquoi ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print('Training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<span style=\"color:red\">To do</span>**:\n",
    "\n",
    "Les trois représentations de base dans ce cours sont:\n",
    " - RGB\n",
    " - Gris\n",
    " - HSV\n",
    " - détection de bord Canny\n",
    " - HOG\n",
    " - SIFT\n",
    "\n",
    "Nous pouvons utiliser une représentation toute seule ou faire de combinaisons pour ajouter caractéristiques supplémentaires qui sont possibles avec différentes représentations. Dans ce cadre, nous pouvons essayer:\n",
    " - RGB\n",
    " - Gris\n",
    " - RGB, Canny\n",
    " - Gris, HOG\n",
    " - RGB, HOG, Canny\n",
    " - SIFT, gris\n",
    " - et d'autres combinaison\n",
    "\n",
    "\n",
    "> * Utilisez les autres méthodes de prétraitement (avec lemmatisation, avec racinisation, etc. `get_RGB_image, get_gray_image, get_HSV_image, get_gray_thresh_binary, get_gray_thresh_binary_inv, get_gray_thresh_trunc, get_gray_thresh_tozero, \n",
    "get_gray_thresh_tozero_inv, get_edges, get_smoothen_image, get_gaussian_blur, get_SIFT_features`) et comparez les résultats de ces trois modèles. Vous devez réécrire le code à partir de \"5.1 Diviser les jeux de données\" et changer la colonne de données. **Lorsque vous utilisez `get_SIFT_features`, appliquez` image = image.flatten()[:1024]`. La sortie de `get_SIFT_features` a des vecteurs de taille différente, nous prendrons un maximum de 1024 valeurs.**\n",
    "> * Réécrivez la méthode de prétraitement avec différentes combinaisons, divisez l'ensemble de données, réduisez la taille avec StandardScaler et PCA entraîne les modèles de régression logistique et SVM. Vérifiez la precision (accuracy et F1-score) des résultats. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    \n",
    "    # resize image\n",
    "    image = cv2.resize(image, (300, 300))\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # we need to work with vectors so we transform the 3D matrix (height, width, channels) in 1D (a list) \n",
    "    image = image.flatten()\n",
    "    \n",
    "    return image\n",
    "\n",
    "data['preprocessed_image'] = data['image'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Nous utilisons ici la colonne qui a été prétraitée.\n",
    "X_train, X_test, y_train, y_test = train_test_split(list(data['preprocessed_image']), data['label'], test_size=0.1)\n",
    "print('Train:', len(X_train), 'images', 'Test:', len(X_test), 'images')\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "X_train = standard_scaler.fit_transform(X_train)\n",
    "X_test = standard_scaler.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=50)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "print('PCA train data matrix shape is: ', X_train.shape)\n",
    "print('PCA test data matrix shape is: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Apprentissage profond (deep learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification avec des réseaux de neurones\n",
    "\n",
    "Les réseaux de neurones peuvent être entraînés pour apprendre à la fois la représentation vectorielle des mots (au lieu de tf-idf) et comment classer les documents. Le code ci-dessous vous permet d'entraîner un classificateur des images neuronal convolutif. La plupart du code est écrit, il suffit de définir l'architecture du réseau avec les bons paramètres avant de l'entraîner:\n",
    "\n",
    "## **<span style=\"color:red\">To do</span>**:\n",
    "\n",
    "> * Aller plus loin. Vérifiez [Image classification from scratch](https://keras.io/examples/vision/image_classification_from_scratch/)\n",
    "> * Comment ce réseau de neurones se compare-t-il aux autres modèles? \n",
    "> * Quelle est la performance?\n",
    "> * Qu'apporte le changement de paramètres dans la performance?\n",
    "> * Utilisez des plongements pré-entraînés et chargez-les en tant que poids dans ce modèle (au lieu de ceux générés aléatoirement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour appliquer des méthodes d'apprentissage en profondeur, pour les images, nous travaillerons avec deux dossiers (`train` et `test`) avec les données qui étaient déjà divisées auparavant. Nous utiliserons la méthode `image_dataset_from_directory` pour charger les images pour l'entraînement et tester le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (180, 180)\n",
    "img_height, img_width = 180, 180\n",
    "batch_size = 32\n",
    "\n",
    "print('Training dataset:')\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"data/Tobacco3482-images/train\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "print('Test dataset:')\n",
    "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"data/Tobacco3482-images/test\",\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'entraînement des réseaux de neurones, nous devons spécifier le nombre de classes pour la couche de prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES_LIST = np.unique(data['label'])\n",
    "n_out = len(CLASSES_LIST)\n",
    "\n",
    "print(CLASSES_LIST, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réseau de neurones convolutifs pour la classification d'images (avec 3 couches convolutives):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "\n",
    "model = Sequential([\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(num_classes) # couche de prédiction\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous préparons le réseau pour l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.cache().shuffle(2021).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "epochs = 10 # train the model for 10 epochs\n",
    "history = model.fit(\n",
    "  train_dataset,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "y_test = []\n",
    "y_pred = []\n",
    "\n",
    "for image, y in test_dataset: # iterate in the test dataset\n",
    "    y_pred += list(model.predict_classes(image)) # add the predictions of the batch to our list\n",
    "    y_test += list(np.argmax(y.numpy(), axis=-1)) # add the true labels to our list\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=CLASSES_LIST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, xticklabels=CLASSES_LIST, yticklabels=CLASSES_LIST)\n",
    "plt.ylabel('True classes')\n",
    "plt.xlabel('Predicted classes')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
